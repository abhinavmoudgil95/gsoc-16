{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://oproject.org/tiki-download_file.php?fileId=8&display&x=450&y=128\" width=\"200\" height=\"200\">\n",
    "<img src=\"http://gfif.udea.edu.co/root/tmva/img/tmva_logo.gif\" width=\"200\" height=\"200\">\n",
    "\n",
    "# Autoencoder Variable Transformation\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader and Factory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TMVA::Tools::Instance();\n",
    "// TString path = \"../datasets/higgs-dataset.root\";\n",
    "// if (gSystem->AccessPathName( path ))  // file does not exist in local directory\n",
    "//         gSystem->Exec(\"curl -O http://files.oproject.org/root/tmva/datasets/higgs/higgs-dataset.root\");\n",
    "// TFile *input = TFile::Open( path );\n",
    "// TMVA::DataLoader *loader2=new TMVA::DataLoader(\"higgs-dataset\");\n",
    "// TTree *Tsignal     = (TTree*)input->Get(\"TreeS\");\n",
    "// TTree *Tbackground = (TTree*)input->Get(\"TreeB\");\n",
    "// TString outfileName( \"higgs_output.root\" );\n",
    "// TFile* outputFile = TFile::Open( outfileName, \"RECREATE\" );\n",
    "// TMVA::Factory *factory = new TMVA::Factory( \"TMVAClassification\", outputFile,\n",
    "//             \"!V:!Silent:Color:DrawProgressBar:AnalysisType=Classification\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// loader2->AddVariable(\"lepton_pT\",'F');\n",
    "// loader2->AddVariable(\"lepton_eta\",'F');\n",
    "// loader2->AddVariable(\"lepton_phi\",'F');\n",
    "// loader2->AddVariable(\"missing_energy_magnitude\",'F');\n",
    "// loader2->AddVariable(\"missing_energy_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_1_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_1_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_1_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_1_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_2_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_2_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_2_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_2_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_3_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_3_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_3_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_3_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_4_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_4_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_4_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_4_b_tag\",'F');\n",
    "// loader2->AddVariable(\"m_jj\",'F');\n",
    "// loader2->AddVariable(\"m_jjj\",'F');\n",
    "// loader2->AddVariable(\"m_lv\",'F');\n",
    "// loader2->AddVariable(\"m_jlv\",'F');\n",
    "// loader2->AddVariable(\"m_bb\",'F');\n",
    "// loader2->AddVariable(\"m_wbb\",'F');\n",
    "// loader2->AddVariable(\"m_wwbb\",'F');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Double_t signalWeight     = 1.0;\n",
    "// Double_t backgroundWeight = 1.0;\n",
    "// loader2->AddSignalTree    ( Tsignal,     signalWeight     );\n",
    "// loader2->AddBackgroundTree( Tbackground, backgroundWeight );\n",
    "// TCut myCuts = \"\"; \n",
    "// TCut myCutb = \"\";\n",
    "// loader2->PrepareTrainingAndTestTree( myCuts, myCutb,\n",
    "//                                          \"nTrain_Signal=1829123:nTrain_Background=1170877:nTest_Signal=4000000:nTest_Background=4000000:SplitMode=Random:NormMode=NumEvents:!V\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// calculates min, max, mean, RMS and variance of all variables\n",
    "// loader2->CalcNorm();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// TString layoutString (\"Layout=TANH|100,TANH|50,TANH|10,LINEAR\");\n",
    "// TString training0 (\"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True\");\n",
    "// TString training1 (\"LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\");\n",
    "// TString training2 (\"LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True\");\n",
    "// TString training3 (\"LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\");\n",
    "\n",
    "// TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "// trainingStrategyString += training0 + \"|\" + training1 + \"|\" + training2 + \"|\" + training3;\n",
    "// // TString nnOptions (\"AE(!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "// // nnOptions.Append (\":\");\n",
    "// // nnOptions.Append (layoutString);\n",
    "// // nnOptions.Append (\":\");\n",
    "// // nnOptions.Append (trainingStrategyString);\n",
    "// // nnOptions.Append (\")\");\n",
    "\n",
    "// TString nnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (layoutString);\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (trainingStrategyString);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// print nnOptions string \n",
    "// cout << nnOptions.Data() << endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Autoencoder Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// // TMVA::DataLoader* newloader = loader2->VarTransform(nnOptions);\n",
    "// factory->BookMethod(loader2, TMVA::Types::kDNN, \"DNN\", nnOptions ); // NN\n",
    "// factory->TrainAllMethods();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// factory->TestAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Factory                  : You are running ROOT Version: 6.07/07, Apr 1, 2016\n",
      "--- Factory                  : \n",
      "--- Factory                  : _/_/_/_/_/ _|      _|  _|      _|    _|_|   \n",
      "--- Factory                  :    _/      _|_|  _|_|  _|      _|  _|    _| \n",
      "--- Factory                  :   _/       _|  _|  _|  _|      _|  _|_|_|_| \n",
      "--- Factory                  :  _/        _|      _|    _|  _|    _|    _| \n",
      "--- Factory                  : _/         _|      _|      _|      _|    _| \n",
      "--- Factory                  : \n",
      "--- Factory                  : ___________TMVA Version 4.2.1, Feb 5, 2015\n",
      "--- Factory                  : \n"
     ]
    }
   ],
   "source": [
    "TMVA::Tools::Instance();\n",
    "TFile *inputFile = TFile::Open( \"../datasets/mydataset.root\"); \n",
    "TFile* outputFile = TFile::Open( \"mydataset_output.root\", \"RECREATE\" );\n",
    "TMVA::Factory *factory = new TMVA::Factory(\"TMVAClassification\", outputFile, \n",
    "                                           \"!V:ROC:!Correlations:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" );\n",
    "TMVA::DataLoader *loader1=new TMVA::DataLoader(\"mydataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataSetInfo              : Dataset[mydataset] : Added class \"Signal\"\t with internal class number 0\n",
      "--- mydataset                : Add Tree MyMCSig of type Signal with 5449 events\n",
      "--- DataSetInfo              : Dataset[mydataset] : Added class \"Background\"\t with internal class number 1\n",
      "--- mydataset                : Add Tree MyMCBkg of type Background with 5449 events\n",
      "--- mydataset                : Preparing trees for training and testing...\n"
     ]
    }
   ],
   "source": [
    "loader1->AddVariable(\"var0\", 'F');\n",
    "loader1->AddVariable(\"var1\", 'F');\n",
    "loader1->AddVariable(\"var2\", 'F');\n",
    "loader1->AddVariable(\"var3 := var0-var1\", 'F');\n",
    "loader1->AddVariable(\"var4 := var0*var2\", 'F');\n",
    "loader1->AddVariable(\"var5 := var1+var2\", 'F');\n",
    "TTree *tsignal = (TTree*)inputFile->Get(\"MyMCSig\");\n",
    "TTree *tbackground = (TTree*)inputFile->Get(\"MyMCBkg\");\n",
    "TCut mycuts = \"\";\n",
    "TCut mycutb = \"\";\n",
    "loader1->AddSignalTree( tsignal,     1.0 );\n",
    "loader1->AddBackgroundTree( tbackground, 1.0 );\n",
    "loader1->PrepareTrainingAndTestTree( mycuts, mycutb,\"nTrain_Signal=3000:nTrain_Background=3000:nTest_Signal=1449:nTest_Background=1449:SplitMode=Random:NormMode=NumEvents:!V\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataSetFactory           : Dataset[mydataset] : Splitmode is: \"RANDOM\" the mixmode is: \"SAMEASSPLITMODE\"\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create training and testing trees -- looping over class \"Signal\" ...\n",
      "--- DataSetFactory           : Dataset[mydataset] : Weight expression for class 'Signal': \"\"\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create training and testing trees -- looping over class \"Background\" ...\n",
      "--- DataSetFactory           : Dataset[mydataset] : Weight expression for class 'Background': \"\"\n",
      "--- DataSetFactory           : Dataset[mydataset] : Number of events in input trees (after possible flattening of arrays):\n",
      "--- DataSetFactory           : Dataset[mydataset] :     Signal          -- number of events       : 5449   / sum of weights: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] :     Background      -- number of events       : 5449   / sum of weights: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] :     Signal     tree -- total number of entries: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] :     Background tree -- total number of entries: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] : Preselection: (will NOT affect number of requested training and testing events)\n",
      "--- DataSetFactory           : Dataset[mydataset] :     No preselection cuts applied on event classes\n",
      "--- DataSetFactory           : Dataset[mydataset] : Weight renormalisation mode: \"NumEvents\": renormalises all event classes \n",
      "--- DataSetFactory           : Dataset[mydataset] :  such that the effective (weighted) number of events in each class equals the respective \n",
      "--- DataSetFactory           : Dataset[mydataset] :  number of events (entries) that you demanded in PrepareTrainingAndTestTree(\"\",\"nTrain_Signal=.. )\n",
      "--- DataSetFactory           : Dataset[mydataset] :  ... i.e. such that Sum[i=1..N_j]{w_i} = N_j, j=0,1,2...\n",
      "--- DataSetFactory           : Dataset[mydataset] :  ... (note that N_j is the sum of TRAINING events (nTrain_j...with j=Signal,Background..\n",
      "--- DataSetFactory           : Dataset[mydataset] :  ..... Testing events are not renormalised nor included in the renormalisation factor! )\n",
      "--- DataSetFactory           : Dataset[mydataset] : --> Rescale Signal     event weights by factor: 1\n",
      "--- DataSetFactory           : Dataset[mydataset] : --> Rescale Background event weights by factor: 1\n",
      "--- DataSetFactory           : Dataset[mydataset] : Number of training and testing events after rescaling:\n",
      "--- DataSetFactory           : Dataset[mydataset] : ---------------------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[mydataset] : Signal     -- training events            : 3000 (sum of weights: 3000) - requested were 3000 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Signal     -- testing events             : 1449 (sum of weights: 1449) - requested were 1449 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Signal     -- training and testing events: 4449 (sum of weights: 4449)\n",
      "--- DataSetFactory           : Dataset[mydataset] : Background -- training events            : 3000 (sum of weights: 3000) - requested were 3000 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Background -- testing events             : 1449 (sum of weights: 1449) - requested were 1449 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Background -- training and testing events: 4449 (sum of weights: 4449)\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create internal training tree\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create internal testing tree\n",
      "--- DataSetInfo              : Dataset[mydataset] : Correlation matrix (Signal):\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetInfo              :               var0    var1    var2 var0-var1 var0*var2 var1+var2\n",
      "--- DataSetInfo              :      var0:  +1.000  -0.008  +0.011    +0.852    +0.922    +0.000\n",
      "--- DataSetInfo              :      var1:  -0.008  +1.000  +0.010    -0.531    -0.004    +0.811\n",
      "--- DataSetInfo              :      var2:  +0.011  +0.010  +1.000    +0.004    +0.335    +0.593\n",
      "--- DataSetInfo              : var0-var1:  +0.852  -0.531  +0.004    +1.000    +0.784    -0.425\n",
      "--- DataSetInfo              : var0*var2:  +0.922  -0.004  +0.335    +0.784    +1.000    +0.192\n",
      "--- DataSetInfo              : var1+var2:  +0.000  +0.811  +0.593    -0.425    +0.192    +1.000\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetInfo              : Dataset[mydataset] : Correlation matrix (Background):\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetInfo              :               var0    var1    var2 var0-var1 var0*var2 var1+var2\n",
      "--- DataSetInfo              :      var0:  +1.000  -0.008  +0.008    +0.650    +0.670    -0.001\n",
      "--- DataSetInfo              :      var1:  -0.008  +1.000  +0.009    -0.766    +0.001    +0.738\n",
      "--- DataSetInfo              :      var2:  +0.008  +0.009  +1.000    -0.002    +0.696    +0.682\n",
      "--- DataSetInfo              : var0-var1:  +0.650  -0.766  -0.002    +1.000    +0.431    -0.561\n",
      "--- DataSetInfo              : var0*var2:  +0.670  +0.001  +0.696    +0.431    +1.000    +0.471\n",
      "--- DataSetInfo              : var1+var2:  -0.001  +0.738  +0.682    -0.561    +0.471    +1.000\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[mydataset] :  \n",
      "--- mydataset                : ----------------------------------------------------------------\n",
      "--- mydataset                : Variables        Variance  \n",
      "--- mydataset                : ----------------------------------------------------------------\n",
      "--- mydataset                : var0             2.8351           \n",
      "--- mydataset                : var1             2.0238           \n",
      "--- mydataset                : var2             1.4587           \n",
      "--- mydataset                : var0-var1        4.9581           \n",
      "--- mydataset                : var0*var2        57.202           \n",
      "--- mydataset                : var1+var2        3.1454           \n",
      "--- mydataset                : ----------------------------------------------------------------\n",
      "--- mydataset                : Targets Variance  \n",
      "--- mydataset                : ----------------------------------------------------------------\n",
      "--- mydataset                : Set minNorm/maxNorm for variables to: \n",
      "--- mydataset                :     var0\t: [0.0434\t, 10\t] \n",
      "--- mydataset                :     var1\t: [9.94e-05\t, 5\t] \n",
      "--- mydataset                :     var2\t: [0.000886\t, 5\t] \n",
      "--- mydataset                :     var0-var1\t: [-4.45\t, 9.96\t] \n",
      "--- mydataset                :     var0*var2\t: [0.000884\t, 48.2\t] \n",
      "--- mydataset                :     var1+var2\t: [0.142\t, 9.87\t] \n",
      "--- mydataset                : Set minNorm/maxNorm for targets to: \n"
     ]
    }
   ],
   "source": [
    "// calculates min, max, mean, RMS and variance of all variables\n",
    "loader1->CalcNorm();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TString layoutString (\"Layout=TANH|100,TANH|50,TANH|10,LINEAR\");\n",
    "TString training0 (\"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True\");\n",
    "TString training1 (\"LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\");\n",
    "TString training2 (\"LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True\");\n",
    "TString training3 (\"LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\");\n",
    "\n",
    "TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "trainingStrategyString += training0 + \"|\" + training1 + \"|\" + training2 + \"|\" + training3;\n",
    "TString nnOptions (\"AE(!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "nnOptions.Append (\":\");\n",
    "nnOptions.Append (layoutString);\n",
    "nnOptions.Append (\":\");\n",
    "nnOptions.Append (trainingStrategyString);\n",
    "nnOptions.Append (\")\");\n",
    "\n",
    "// TString nnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (layoutString);\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (trainingStrategyString);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DNN                      : Parsing option string: \n",
      "--- DNN                      : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM:Layout=TANH|100,TANH|50,TANH|10,LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True|LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1|LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True|LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\"\n",
      "--- DNN                      : The following options are set:\n",
      "--- DNN                      : - By User:\n",
      "--- DNN                      :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "--- DNN                      :     VarTransform: \"G\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "--- DNN                      :     H: \"False\" [Print method-specific help message]\n",
      "--- DNN                      :     Layout: \"TANH|100,TANH|50,TANH|10,LINEAR\" [neural network layout]\n",
      "--- DNN                      :     ErrorStrategy: \"CROSSENTROPY\" [error strategy (regression: sum of squares; classification: crossentropy; multiclass: crossentropy/mutual exclusive cross entropy]\n",
      "--- DNN                      :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "--- DNN                      :     TrainingStrategy: \"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True|LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1|LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True|LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\" [defines the training strategies]\n",
      "--- DNN                      : - Default:\n",
      "--- DNN                      :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "--- DNN                      :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "--- DNN                      :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "--- DNN                      :     SignalWeightsSum: \"1.000000e+03\" [Sum of weights of signal; Is used to compute the significance on the fly]\n",
      "--- DNN                      :     BackgroundWeightsSum: \"1.000000e+03\" [Sum of weights of background; Is used to compute the significance on the fly]\n",
      "--- DNN                      : Dataset[mydataset] : Create Transformation \"G\" with events from all classes.\n",
      "--- Gauss                    : Transformation, Variable selection : \n",
      "--- Gauss                    : Input : variable 'var0' (index=0).   <---> Output : variable 'var0' (index=0).\n",
      "--- Gauss                    : Input : variable 'var1' (index=1).   <---> Output : variable 'var1' (index=1).\n",
      "--- Gauss                    : Input : variable 'var2' (index=2).   <---> Output : variable 'var2' (index=2).\n",
      "--- Gauss                    : Input : variable 'var3' (index=3).   <---> Output : variable 'var3' (index=3).\n",
      "--- Gauss                    : Input : variable 'var4' (index=4).   <---> Output : variable 'var4' (index=4).\n",
      "--- Gauss                    : Input : variable 'var5' (index=5).   <---> Output : variable 'var5' (index=5).\n",
      "--- mydataset                : Train method: DNN for Classification\n",
      "--- Gauss                    : Preparing the Gaussian transformation...\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            : Variable        Mean        RMS   [        Min        Max ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            :     var0:   0.012528     1.0099   [    -3.0336     5.7307 ]\n",
      "--- TFHandler_DNN            :     var1:   0.011561     1.0064   [    -2.9775     5.7307 ]\n",
      "--- TFHandler_DNN            :     var2:   0.011117     1.0050   [    -3.0332     5.7307 ]\n",
      "--- TFHandler_DNN            :     var3:   0.011715     1.0060   [    -3.0339     5.7307 ]\n",
      "--- TFHandler_DNN            :     var4:   0.012320     1.0100   [    -3.0336     5.7307 ]\n",
      "--- TFHandler_DNN            :     var5:   0.010124    0.99939   [    -3.0337     5.7307 ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- DNN                      : Dataset[mydataset] : Begin training\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            : Variable        Mean        RMS   [        Min        Max ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            :     var0:   0.012528     1.0099   [    -3.0336     5.7307 ]\n",
      "--- TFHandler_DNN            :     var1:   0.011561     1.0064   [    -2.9775     5.7307 ]\n",
      "--- TFHandler_DNN            :     var2:   0.011117     1.0050   [    -3.0332     5.7307 ]\n",
      "--- TFHandler_DNN            :     var3:   0.011715     1.0060   [    -3.0339     5.7307 ]\n",
      "--- TFHandler_DNN            :     var4:   0.012320     1.0100   [    -3.0336     5.7307 ]\n",
      "--- TFHandler_DNN            :     var5:   0.010124    0.99939   [    -3.0337     5.7307 ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            : Variable        Mean        RMS   [        Min        Max ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- TFHandler_DNN            :     var0:  -0.017280     1.0156   [    -3.0319     5.7307 ]\n",
      "--- TFHandler_DNN            :     var1:   0.056124     1.0052   [    -2.9665     5.7307 ]\n",
      "--- TFHandler_DNN            :     var2:  -0.024552    0.98190   [    -3.0355     5.7307 ]\n",
      "--- TFHandler_DNN            :     var3:  -0.026541    0.98439   [    -2.7972     5.7307 ]\n",
      "--- TFHandler_DNN            :     var4:  -0.023172     1.0047   [    -3.0332     5.7307 ]\n",
      "--- TFHandler_DNN            :     var5:   0.024953     1.0417   [    -2.9008     5.7307 ]\n",
      "--- TFHandler_DNN            : -----------------------------------------------------------\n",
      "--- DNN                      : Add Layer with 100 nodes.\n",
      "--- DNN                      : Add Layer with 50 nodes.\n",
      "--- DNN                      : Add Layer with 10 nodes.\n",
      "--- DNN                      : Add Layer with 1 nodes.\n",
      "--- DNN                      : \n",
      "--- DNN                      : Total number of Synapses = 6210\n",
      "--- DNN                      : Training with learning rate = 0.1, momentum = 0, repetitions = 1\n",
      "--- DNN                      : Drop configuration\n",
      "--- DNN                      :     drop repetitions = 1\n",
      "--- DNN                      :     Layer 0 = 0\n",
      "--- DNN                      :     Layer 1 = 0.5\n",
      "--- DNN                      :     Layer 2 = 0.5\n",
      "--- DNN                      :     Layer 3 = 0.5\n",
      "--- DNN                      : \n",
      "\n",
      "learning rate reduced to 0.05\n",
      "\n",
      "learning rate reduced to 0.025\n",
      "\n",
      "learning rate reduced to 0.0125\n",
      "\n",
      "learning rate reduced to 0.00625\n",
      "--- DNN                      : \n",
      "--- DNN                      : Training with learning rate = 0.01, momentum = 0.5, repetitions = 1\n",
      "--- DNN                      : Drop configuration\n",
      "--- DNN                      :     drop repetitions = 1\n",
      "--- DNN                      :     Layer 0 = 0\n",
      "--- DNN                      :     Layer 1 = 0.1\n",
      "--- DNN                      :     Layer 2 = 0.1\n",
      "--- DNN                      :     Layer 3 = 0.1\n",
      "--- DNN                      : \n",
      "--- DNN                      : \n",
      "--- DNN                      : Training with learning rate = 0.01, momentum = 0.3, repetitions = 1\n",
      "--- DNN                      : \n",
      "--- DNN                      : \n",
      "--- DNN                      : Training with learning rate = 0.001, momentum = 0.1, repetitions = 1\n",
      "--- DNN                      : \n",
      "--- DNN                      : \n",
      "--- DNN                      : Dataset[mydataset] : End of training                                              \n",
      "--- DNN                      : Dataset[mydataset] : Elapsed time for training with 6000 events: \u001b[1;31m939 sec\u001b[0m         \n",
      "--- DNN                      : Dataset[mydataset] : Create MVA output for Dataset[mydataset] : classification on training sample\n",
      "--- DNN                      : Dataset[mydataset] : Evaluation of DNN on training sample (6000 events)\n",
      "--- DNN                      : Dataset[mydataset] : Elapsed time for evaluation of 6000 events: \u001b[1;31m0.0656 sec\u001b[0m       \n",
      "--- DNN                      : Dataset[mydataset] : Creating weight file in xml format: \u001b[0;36mmydataset/weights/TMVAClassification_DNN.weights.xml\u001b[0m\n",
      "--- DNN                      : Dataset[mydataset] : Creating standalone response class: \u001b[0;36mmydataset/weights/TMVAClassification_DNN.class.C\u001b[0m\n",
      "--- DNN                      : mydataset_output.root:/mydataset/Method_DNN/DNN\n",
      "--- mydataset                : Training finished\n"
     ]
    }
   ],
   "source": [
    "TMVA::DataLoader* newloader = loader1->VarTransform(nnOptions);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

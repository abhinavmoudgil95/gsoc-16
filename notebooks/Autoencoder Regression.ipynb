{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://oproject.org/tiki-download_file.php?fileId=8&display&x=450&y=128\" width=\"200\" height=\"200\">\n",
    "<img src=\"http://gfif.udea.edu.co/root/tmva/img/tmva_logo.gif\" width=\"200\" height=\"200\">\n",
    "\n",
    "# Autoencoder Variable Transformation\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DataLoader and Factory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Factory                  : You are running ROOT Version: 6.07/07, Apr 1, 2016\n",
      "--- Factory                  : \n",
      "--- Factory                  : _/_/_/_/_/ _|      _|  _|      _|    _|_|   \n",
      "--- Factory                  :    _/      _|_|  _|_|  _|      _|  _|    _| \n",
      "--- Factory                  :   _/       _|  _|  _|  _|      _|  _|_|_|_| \n",
      "--- Factory                  :  _/        _|      _|    _|  _|    _|    _| \n",
      "--- Factory                  : _/         _|      _|      _|      _|    _| \n",
      "--- Factory                  : \n",
      "--- Factory                  : ___________TMVA Version 4.2.1, Feb 5, 2015\n",
      "--- Factory                  : \n"
     ]
    }
   ],
   "source": [
    "TMVA::Tools::Instance();\n",
    "TFile *inputFile = TFile::Open( \"../datasets/mydataset.root\"); \n",
    "TFile* outputFile = TFile::Open( \"mydataset_output.root\", \"RECREATE\" );\n",
    "TMVA::Factory *factory = new TMVA::Factory(\"TMVARegression\", outputFile, \n",
    "                                           \"!V:!Silent:Color:DrawProgressBar:AnalysisType=Regression\" );\n",
    "TMVA::DataLoader *loader=new TMVA::DataLoader(\"mydataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataSetInfo              : Dataset[mydataset] : Added class \"Regression\"\t with internal class number 0\n",
      "--- mydataset                : Add Tree MyMCSig of type Regression with 5449 events\n",
      "--- DataSetInfo              : Dataset[mydataset] : Class index : 0  name : Regression\n"
     ]
    }
   ],
   "source": [
    "TMVA::DataLoader *loader2=new TMVA::DataLoader(\"mydataset\");\n",
    "loader->AddVariable(\"var0\", 'F');\n",
    "loader->AddVariable(\"var1\", 'F');\n",
    "loader->AddVariable(\"var2\", 'F');\n",
    "loader->AddVariable(\"var3 := var0-var1\", 'F');\n",
    "loader->AddVariable(\"var4 := var0*var2\", 'F');\n",
    "loader->AddVariable(\"var5 := var1+var2\", 'F');\n",
    "loader->AddTarget(\"var6 := 2*var0*var0+4*var1+5*var2\", \"F\");\n",
    "TTree *regtree = (TTree*)inputFile->Get(\"MyMCSig\");\n",
    "loader->AddRegressionTree(regtree, 1.0);\n",
    "loader->PrepareTrainingAndTestTree(\"\", \"SplitMode=random:!V\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup DNN for Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(indexLayer=1;pretraining=false;!H:V:ErrorStrategy=SUMOFSQUARES:VarTransform=G:WeightInitialization=XAVIERUNIFORM:Layout=TANH|3,LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True|LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1)\n"
     ]
    }
   ],
   "source": [
    "TString layoutString (\"Layout=TANH|3,LINEAR\");\n",
    "TString training0 (\"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True\");\n",
    "TString training1 (\"LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\");\n",
    "// TString training2 (\"LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True\");\n",
    "// TString training3 (\"LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\");\n",
    "\n",
    "// TString training0 (\"LearningRate=1e-5,Momentum=0.5,Repetitions=1,ConvergenceSteps=500,BatchSize=50,TestRepetitions=7,WeightDecay=0.01,Regularization=NONE,DropConfig=0.5+0.5+0.5+0.5,DropRepetitions=2,Multithreading=True\");\n",
    "// TString training1 (\"LearningRate=1e-5,Momentum=0.9,Repetitions=1,ConvergenceSteps=170,BatchSize=30,TestRepetitions=7,WeightDecay=0.01,Regularization=L2,DropConfig=0.1+0.1+0.1,DropRepetitions=1,Multithreading=True\");\n",
    "// TString training2 (\"LearningRate=1e-5,Momentum=0.3,Repetitions=1,ConvergenceSteps=150,BatchSize=40,TestRepetitions=7,WeightDecay=0.01,Regularization=NONE,Multithreading=True\");\n",
    "// TString training3 (\"LearningRate=1e-6,Momentum=0.1,Repetitions=1,ConvergenceSteps=500,BatchSize=100,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\");\n",
    "\n",
    "TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "// trainingStrategyString += training0 + \"|\" + training1 + \"|\" + training2 + \"|\" + training3;\n",
    "trainingStrategyString += training0 + \"|\" + training1;\n",
    "TString nnOptions (\"AE(indexLayer=1;pretraining=false;!H:V:ErrorStrategy=SUMOFSQUARES:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "nnOptions.Append (\":\");\n",
    "nnOptions.Append (layoutString);\n",
    "nnOptions.Append (\":\");\n",
    "nnOptions.Append (trainingStrategyString);\n",
    "nnOptions.Append (\")\");\n",
    "\n",
    "cout << nnOptions.Data() << endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Autoencoder Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mydataset                : [AE Transform] indexLayer = 1\n",
      "--- mydataset                : [AE Transform] Pretraining = false\n",
      "--- mydataset                : Preparing DataLoader for Autoencoder Transform DNN Training\n",
      "--- DataSetFactory           : Dataset[mydataset] : Splitmode is: \"RANDOM\" the mixmode is: \"SAMEASSPLITMODE\"\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create training and testing trees -- looping over class \"Regression\" ...\n",
      "--- DataSetFactory           : Dataset[mydataset] : Weight expression for class 'Regression': \"\"\n",
      "--- DataSetFactory           : Dataset[mydataset] : Number of events in input trees (after possible flattening of arrays):\n",
      "--- DataSetFactory           : Dataset[mydataset] :     Regression      -- number of events       : 5449   / sum of weights: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] :     Regression tree -- total number of entries: 5449 \n",
      "--- DataSetFactory           : Dataset[mydataset] : Preselection: (will NOT affect number of requested training and testing events)\n",
      "--- DataSetFactory           : Dataset[mydataset] :     No preselection cuts applied on event classes\n",
      "--- DataSetFactory           : Dataset[mydataset] : Weight renormalisation mode: \"EqualNumEvents\": renormalises all event classes ...\n",
      "--- DataSetFactory           : Dataset[mydataset] :  such that the effective (weighted) number of events in each class is the same \n",
      "--- DataSetFactory           : Dataset[mydataset] :  (and equals the number of events (entries) given for class=0 )\n",
      "--- DataSetFactory           : Dataset[mydataset] : ... i.e. such that Sum[i=1..N_j]{w_i} = N_classA, j=classA, classB, ...\n",
      "--- DataSetFactory           : Dataset[mydataset] : ... (note that N_j is the sum of TRAINING events\n",
      "--- DataSetFactory           : Dataset[mydataset] :  ..... Testing events are not renormalised nor included in the renormalisation factor!)\n",
      "--- DataSetFactory           : Dataset[mydataset] : --> Rescale Regression event weights by factor: 1\n",
      "--- DataSetFactory           : Dataset[mydataset] : Number of training and testing events after rescaling:\n",
      "--- DataSetFactory           : Dataset[mydataset] : ---------------------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[mydataset] : Regression -- training events            : 2724 (sum of weights: 2724) - requested were 0 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Regression -- testing events             : 2724 (sum of weights: 2724) - requested were 0 events\n",
      "--- DataSetFactory           : Dataset[mydataset] : Regression -- training and testing events: 5448 (sum of weights: 5448)\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create internal training tree\n",
      "--- DataSetFactory           : Dataset[mydataset] : Create internal testing tree\n",
      "--- DataSetInfo              : Dataset[mydataset] : Correlation matrix (Regression):\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetInfo              :               var0    var1    var2 var0-var1 var0*var2 var1+var2\n",
      "--- DataSetInfo              :      var0:  +1.000  -0.017  +0.005    +0.859    +0.923    -0.011\n",
      "--- DataSetInfo              :      var1:  -0.017  +1.000  +0.021    -0.527    -0.009    +0.816\n",
      "--- DataSetInfo              :      var2:  +0.005  +0.021  +1.000    -0.007    +0.324    +0.595\n",
      "--- DataSetInfo              : var0-var1:  +0.859  -0.527  -0.007    +1.000    +0.789    -0.427\n",
      "--- DataSetInfo              : var0*var2:  +0.923  -0.009  +0.324    +0.789    +1.000    +0.180\n",
      "--- DataSetInfo              : var1+var2:  -0.011  +0.816  +0.595    -0.427    +0.180    +1.000\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[mydataset] :  \n",
      "--- DataSetInfo              : Dataset[temporary_loader_for_training] : Added class \"Regression\"\t with internal class number 0\n",
      "--- temporary_loader_for_t...: Add Tree Dataset of type Regression with 2724 events\n",
      "--- DataSetInfo              : Dataset[temporary_loader_for_training] : Class index : 0  name : Regression\n",
      "--- DNN                      : Parsing option string: \n",
      "--- DNN                      : ... \"!H:V:ErrorStrategy=SUMOFSQUARES:VarTransform=G:WeightInitialization=XAVIERUNIFORM:Layout=TANH|3,LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True|LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\"\n",
      "--- DNN                      : The following options are set:\n",
      "--- DNN                      : - By User:\n",
      "--- DNN                      :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "--- DNN                      :     VarTransform: \"G\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "--- DNN                      :     H: \"False\" [Print method-specific help message]\n",
      "--- DNN                      :     Layout: \"TANH|3,LINEAR\" [neural network layout]\n",
      "--- DNN                      :     ErrorStrategy: \"SUMOFSQUARES\" [error strategy (regression: sum of squares; classification: crossentropy; multiclass: crossentropy/mutual exclusive cross entropy]\n",
      "--- DNN                      :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "--- DNN                      :     TrainingStrategy: \"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True|LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\" [defines the training strategies]\n",
      "--- DNN                      : - Default:\n",
      "--- DNN                      :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "--- DNN                      :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "--- DNN                      :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "--- DNN                      :     SignalWeightsSum: \"1.000000e+03\" [Sum of weights of signal; Is used to compute the significance on the fly]\n",
      "--- DNN                      :     BackgroundWeightsSum: \"1.000000e+03\" [Sum of weights of background; Is used to compute the significance on the fly]\n",
      "--- DNN                      : Dataset[temporary_loader_for_training] : Create Transformation \"G\" with events from all classes.\n",
      "--- Gauss                    : Transformation, Variable selection : \n",
      "--- Gauss                    : Input : variable 'var0' (index=0).   <---> Output : variable 'var0' (index=0).\n",
      "--- Gauss                    : Input : variable 'var1' (index=1).   <---> Output : variable 'var1' (index=1).\n",
      "--- Gauss                    : Input : variable 'var2' (index=2).   <---> Output : variable 'var2' (index=2).\n",
      "--- Gauss                    : Input : variable 'var0-var1' (index=3).   <---> Output : variable 'var0-var1' (index=3).\n",
      "--- Gauss                    : Input : variable 'var0*var2' (index=4).   <---> Output : variable 'var0*var2' (index=4).\n",
      "--- Gauss                    : Input : variable 'var1+var2' (index=5).   <---> Output : variable 'var1+var2' (index=5).\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Splitmode is: \"RANDOM\" the mixmode is: \"SAMEASSPLITMODE\"\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Create training and testing trees -- looping over class \"Regression\" ...\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Weight expression for class 'Regression': \"\"\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Number of events in input trees (after possible flattening of arrays):\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :     Regression      -- number of events       : 2724   / sum of weights: 2724 \n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :     Regression tree -- total number of entries: 2724 \n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Preselection: (will NOT affect number of requested training and testing events)\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :     No preselection cuts applied on event classes\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Weight renormalisation mode: \"EqualNumEvents\": renormalises all event classes ...\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :  such that the effective (weighted) number of events in each class is the same \n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :  (and equals the number of events (entries) given for class=0 )\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : ... i.e. such that Sum[i=1..N_j]{w_i} = N_classA, j=classA, classB, ...\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : ... (note that N_j is the sum of TRAINING events\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :  ..... Testing events are not renormalised nor included in the renormalisation factor!)\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : --> Rescale Regression event weights by factor: 1\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Number of training and testing events after rescaling:\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : ---------------------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Regression -- training events            : 1362 (sum of weights: 1362) - requested were 0 events\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Regression -- testing events             : 1362 (sum of weights: 1362) - requested were 0 events\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Regression -- training and testing events: 2724 (sum of weights: 2724)\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Create internal training tree\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] : Create internal testing tree\n",
      "--- DataSetInfo              : Dataset[temporary_loader_for_training] : Correlation matrix (Regression):\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetInfo              :               var0    var1    var2 var0-var1 var0*var2 var1+var2\n",
      "--- DataSetInfo              :      var0:  +1.000  -0.016  -0.009    +0.857    +0.919    -0.018\n",
      "--- DataSetInfo              :      var1:  -0.016  +1.000  +0.036    -0.530    -0.000    +0.824\n",
      "--- DataSetInfo              :      var2:  -0.009  +0.036  +1.000    -0.026    +0.319    +0.597\n",
      "--- DataSetInfo              : var0-var1:  +0.857  -0.530  -0.026    +1.000    +0.780    -0.440\n",
      "--- DataSetInfo              : var0*var2:  +0.919  -0.000  +0.319    +0.780    +1.000    +0.181\n",
      "--- DataSetInfo              : var1+var2:  -0.018  +0.824  +0.597    -0.440    +0.181    +1.000\n",
      "--- DataSetInfo              : ----------------------------------------------------------------\n",
      "--- DataSetFactory           : Dataset[temporary_loader_for_training] :  \n",
      "--- Gauss                    : Preparing the Gaussian transformation...\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :  Variable         Mean         RMS   [        Min         Max ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :      var0:   0.025147    0.99543   [    -2.5576     5.7307 ]\n",
      "--- TFHandler_DNN            :      var1:   0.021436    0.98227   [    -2.5407     5.7307 ]\n",
      "--- TFHandler_DNN            :      var2:   0.023215    0.98505   [    -2.5576     5.7307 ]\n",
      "--- TFHandler_DNN            : var0-var1:   0.029294    0.98538   [    -2.5566     5.7307 ]\n",
      "--- TFHandler_DNN            : var0*var2:   0.029233    0.98911   [    -2.5567     5.7307 ]\n",
      "--- TFHandler_DNN            : var1+var2:   0.021056    0.98191   [    -2.5577     5.7307 ]\n",
      "--- TFHandler_DNN            :      var0:     3.1658     2.0839   [   0.043380     9.9950 ]\n",
      "--- TFHandler_DNN            :      var1:     1.1034     1.2675   [ 9.9424e-05     4.9932 ]\n",
      "--- TFHandler_DNN            :      var2:     3.9814    0.89590   [   0.056750     4.9985 ]\n",
      "--- TFHandler_DNN            : var0-var1:     2.0624     2.4566   [   -0.62489     9.9615 ]\n",
      "--- TFHandler_DNN            : var0*var2:     12.588     8.9608   [    0.16060     48.220 ]\n",
      "--- TFHandler_DNN            : var1+var2:     5.0849     1.5784   [    0.27852     9.7154 ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- DNN                      : Dataset[temporary_loader_for_training] : Begin training\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :  Variable         Mean         RMS   [        Min         Max ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :      var0:   0.025147    0.99543   [    -2.5576     5.7307 ]\n",
      "--- TFHandler_DNN            :      var1:   0.021436    0.98227   [    -2.5407     5.7307 ]\n",
      "--- TFHandler_DNN            :      var2:   0.023215    0.98505   [    -2.5576     5.7307 ]\n",
      "--- TFHandler_DNN            : var0-var1:   0.029294    0.98538   [    -2.5566     5.7307 ]\n",
      "--- TFHandler_DNN            : var0*var2:   0.029233    0.98911   [    -2.5567     5.7307 ]\n",
      "--- TFHandler_DNN            : var1+var2:   0.021056    0.98191   [    -2.5577     5.7307 ]\n",
      "--- TFHandler_DNN            :      var0:     3.1658     2.0839   [   0.043380     9.9950 ]\n",
      "--- TFHandler_DNN            :      var1:     1.1034     1.2675   [ 9.9424e-05     4.9932 ]\n",
      "--- TFHandler_DNN            :      var2:     3.9814    0.89590   [   0.056750     4.9985 ]\n",
      "--- TFHandler_DNN            : var0-var1:     2.0624     2.4566   [   -0.62489     9.9615 ]\n",
      "--- TFHandler_DNN            : var0*var2:     12.588     8.9608   [    0.16060     48.220 ]\n",
      "--- TFHandler_DNN            : var1+var2:     5.0849     1.5784   [    0.27852     9.7154 ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :  Variable         Mean         RMS   [        Min         Max ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- TFHandler_DNN            :      var0:  0.0099397    0.98699   [    -2.3832     5.7307 ]\n",
      "--- TFHandler_DNN            :      var1:  0.0017541    0.96773   [    -2.3898     5.7307 ]\n",
      "--- TFHandler_DNN            :      var2:  -0.078931    0.96072   [    -2.5646     5.7307 ]\n",
      "--- TFHandler_DNN            : var0-var1:   0.016731     1.0015   [    -5.7307     3.3763 ]\n",
      "--- TFHandler_DNN            : var0*var2:  -0.012634    0.98849   [    -2.4987     3.5316 ]\n",
      "--- TFHandler_DNN            : var1+var2:  -0.046298    0.97221   [    -2.5781     5.7307 ]\n",
      "--- TFHandler_DNN            :      var0:     3.1541     2.1026   [    0.10541     9.9950 ]\n",
      "--- TFHandler_DNN            :      var1:     1.0847     1.2567   [ 0.00064853     4.9619 ]\n",
      "--- TFHandler_DNN            :      var2:     3.8989    0.91860   [   0.053113     4.9992 ]\n",
      "--- TFHandler_DNN            : var0-var1:     2.0694     2.4684   [   -0.95297     9.9365 ]\n",
      "--- TFHandler_DNN            : var0*var2:     12.331     8.9232   [    0.19058     48.019 ]\n",
      "--- TFHandler_DNN            : var1+var2:     4.9836     1.5602   [    0.25497     9.7770 ]\n",
      "--- TFHandler_DNN            : ----------------------------------------------------------------\n",
      "--- DNN                      : Add Layer with 3 nodes.\n",
      "--- DNN                      : Add Layer with 6 nodes.\n",
      "--- DNN                      : \n",
      "--- DNN                      : Total number of Synapses = 39\n",
      "--- DNN                      : Training with learning rate = 0.1, momentum = 0, repetitions = 1\n",
      "--- DNN                      : Drop configuration\n",
      "--- DNN                      :     drop repetitions = 1\n",
      "--- DNN                      :     Layer 0 = 0\n",
      "--- DNN                      :     Layer 1 = 0.5\n",
      "--- DNN                      :     Layer 2 = 0.5\n",
      "--- DNN                      :     Layer 3 = 0.5\n",
      "--- DNN                      : \n",
      "\n",
      "learning rate reduced to \n",
      "learning rate reduced to 00..0052\n",
      "5\n",
      "\n",
      "learning rate reduced to \n",
      "learning rat0e\n",
      ". l0re1ea2dr5un\n",
      "ciendg  troa te reduc0e.d0 0t3o1 25\n",
      "0.003125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate reduced to 0.0015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate reduced to 0.00078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DNN                      : \n",
      "--- DNN                      : Training with learning rate = 0.01, momentum = 0.5, repetitions = 1\n",
      "--- DNN                      : Drop configuration\n",
      "--- DNN                      :     drop repetitions = 1\n",
      "--- DNN                      :     Layer 0 = 0\n",
      "--- DNN                      :     Layer 1 = 0.1\n",
      "--- DNN                      :     Layer 2 = 0.1\n",
      "--- DNN                      :     Layer 3 = 0.1\n",
      "--- DNN                      : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate reduced to 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate reduced to \n",
      "learning rate reduced to 0.0025\n",
      "0.00125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "TMVA::DataLoader* newloader = loader->VarTransform(nnOptions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Check number of variables in new Loader\n",
    "newloader->GetDataSetInfo().GetNVariables();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Higgs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TMVA::Tools::Instance();\n",
    "// TString path = \"../datasets/higgs-dataset.root\";\n",
    "// if (gSystem->AccessPathName( path ))  // file does not exist in local directory\n",
    "//         gSystem->Exec(\"curl -O http://files.oproject.org/root/tmva/datasets/higgs/higgs-dataset.root\");\n",
    "// TFile *input = TFile::Open( path );\n",
    "// TMVA::DataLoader *loader2=new TMVA::DataLoader(\"higgs-dataset\");\n",
    "// TTree *Tsignal     = (TTree*)input->Get(\"TreeS\");\n",
    "// TTree *Tbackground = (TTree*)input->Get(\"TreeB\");\n",
    "// TString outfileName( \"higgs_output.root\" );\n",
    "// TFile* outputFile = TFile::Open( outfileName, \"RECREATE\" );\n",
    "// TMVA::Factory *factory = new TMVA::Factory( \"TMVAClassification\", outputFile,\n",
    "//             \"!V:!Silent:Color:DrawProgressBar:AnalysisType=Classification\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// loader2->AddVariable(\"lepton_pT\",'F');\n",
    "// loader2->AddVariable(\"lepton_eta\",'F');\n",
    "// loader2->AddVariable(\"lepton_phi\",'F');\n",
    "// loader2->AddVariable(\"missing_energy_magnitude\",'F');\n",
    "// loader2->AddVariable(\"missing_energy_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_1_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_1_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_1_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_1_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_2_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_2_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_2_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_2_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_3_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_3_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_3_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_3_b_tag\",'F');\n",
    "// loader2->AddVariable(\"jet_4_pt\",'F');\n",
    "// loader2->AddVariable(\"jet_4_eta\",'F');\n",
    "// loader2->AddVariable(\"jet_4_phi\",'F');\n",
    "// loader2->AddVariable(\"jet_4_b_tag\",'F');\n",
    "// loader2->AddVariable(\"m_jj\",'F');\n",
    "// loader2->AddVariable(\"m_jjj\",'F');\n",
    "// loader2->AddVariable(\"m_lv\",'F');\n",
    "// loader2->AddVariable(\"m_jlv\",'F');\n",
    "// loader2->AddVariable(\"m_bb\",'F');\n",
    "// loader2->AddVariable(\"m_wbb\",'F');\n",
    "// loader2->AddVariable(\"m_wwbb\",'F');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Double_t signalWeight     = 1.0;\n",
    "// Double_t backgroundWeight = 1.0;\n",
    "// loader2->AddSignalTree    ( Tsignal,     signalWeight     );\n",
    "// loader2->AddBackgroundTree( Tbackground, backgroundWeight );\n",
    "// TCut myCuts = \"\"; \n",
    "// TCut myCutb = \"\";\n",
    "// loader2->PrepareTrainingAndTestTree( myCuts, myCutb,\n",
    "//                                          \"nTrain_Signal=1829123:nTrain_Background=1170877:nTest_Signal=4000000:nTest_Background=4000000:SplitMode=Random:NormMode=NumEvents:!V\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// calculates min, max, mean, RMS and variance of all variables\n",
    "// loader2->CalcNorm();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// TString layoutString (\"Layout=TANH|100,TANH|50,TANH|10,LINEAR\");\n",
    "// TString training0 (\"LearningRate=1e-1,Momentum=0.0,Repetitions=1,ConvergenceSteps=300,BatchSize=20,TestRepetitions=15,WeightDecay=0.001,Regularization=NONE,DropConfig=0.0+0.5+0.5+0.5,DropRepetitions=1,Multithreading=True\");\n",
    "// TString training1 (\"LearningRate=1e-2,Momentum=0.5,Repetitions=1,ConvergenceSteps=300,BatchSize=30,TestRepetitions=7,WeightDecay=0.001,Regularization=L2,Multithreading=True,DropConfig=0.0+0.1+0.1+0.1,DropRepetitions=1\");\n",
    "// TString training2 (\"LearningRate=1e-2,Momentum=0.3,Repetitions=1,ConvergenceSteps=300,BatchSize=40,TestRepetitions=7,WeightDecay=0.0001,Regularization=L2,Multithreading=True\");\n",
    "// TString training3 (\"LearningRate=1e-3,Momentum=0.1,Repetitions=1,ConvergenceSteps=200,BatchSize=70,TestRepetitions=7,WeightDecay=0.0001,Regularization=NONE,Multithreading=True\");\n",
    "\n",
    "// TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "// trainingStrategyString += training0 + \"|\" + training1 + \"|\" + training2 + \"|\" + training3;\n",
    "// // TString nnOptions (\"AE(!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "// // nnOptions.Append (\":\");\n",
    "// // nnOptions.Append (layoutString);\n",
    "// // nnOptions.Append (\":\");\n",
    "// // nnOptions.Append (trainingStrategyString);\n",
    "// // nnOptions.Append (\")\");\n",
    "\n",
    "// TString nnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIERUNIFORM\");\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (layoutString);\n",
    "// nnOptions.Append (\":\");\n",
    "// nnOptions.Append (trainingStrategyString);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// print nnOptions string \n",
    "// cout << nnOptions.Data() << endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// TMVA::DataLoader* newloader2 = loader2->VarTransform(nnOptions);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
